#+SETUPFILE: ../tools/template.org
#+TITLE: 6 Normal Equation
* Analytical Solution
There exists another way to solve for the weights using linear algebra in one step. It is reminiscent of calculus's min/max method:
$$ \frac{d}{d\theta} J(\theta) = 0 \qquad \text{Solve for the vector }\theta$$
Instead of solving for the actual hypothesis function we would instead fill in all the numbers for each training pair. With just the numbers in a matrix form, we can find the best $\theta$ by using vector projection.
** Equation
We'll be looking to solve the following equation:
$$X \theta = y$$
*NOTE*: We will follow the coursera method, but the design matrix could also be re-orginsed and re-written in a different way: $\theta^T X = y$
** Full equation
Here, $X$ is our input and $y$ is our output. We're trying to find the best projection $\theta$ to map from our input space to our output.
$$ \begin{bmatrix}
x_0^1  & x_1^1  & \cdots & x_n^1  \\
x_0^2  & x_1^2  & \cdots & x_n^2  \\
x_0^3  & x_1^3  & \cdots & x_n^3  \\
\vdots & \vdots & \ddots & \vdots \\
x_0^m  & x_1^m  & \cdots & x_n^m 
\end{bmatrix}
\begin{bmatrix}
\theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n 
\end{bmatrix} = 
\begin{bmatrix}
y^0 \\ y^1 \\ y^2 \\ \vdots \\ y^m 
\end{bmatrix}$$
** Solving for $\theta$
1. Turn the design matrix $X$ into a square matrix to find the inverse.
2. Multiply the transpose $X^T$ to each side
3. Find the inverse and multiply to each side
$$\begin{align*}
X \theta                    &= y                  \\
X^T X \theta                &= X^T y              \\
(X^T X)^{-1} (X^T X) \theta &= (X^T X)^{-1} X^T y \\
                     \theta &= (X^T X)^{-1} X^T y \\
\end{align*}$$
** Cases for Noninvertability
Sometimes $X^TX$ is noninvertible
+ Two or more redundant features
  + very closely related
  + linearly dependent
+ Too many features not enough training points
  + $n \leq m$
  + regularization method to be applied
For solving use $\text{pinv}$ instead of $\text{inv}$. Stands for pseudo-inverse.
#+BEGIN_SRC matlab
theta = pinv(X'*X)*X'*y;
#+END_SRC
* When to use the normal equation
| *Gradient Descent*        | *Normal Equation*               |
| <c>                       | <c>                             |
|---------------------------+---------------------------------|
| Feature scalling          | No feature scalling             |
| Need to choose \alpha     | No need to choose \alpha        |
| Needs multiple iterations | Iteration isn't need            |
| $O(kn^2)$                 | $O(n^3)$ because of $(X^TX)^-1$ |
| Works great for large $n$ | Slow if $n > 10000$             |
